
\documentclass{article}
\usepackage[margin=0.5in]{geometry}
\begin{document}
\title{Regression Tree - Framingham Heart Study data}
\author{Sangsoo Park (D)}
\maketitle

\section{What is Regression Tree?}

  Regression tree analysis has been used to understand underlying structures of data to predict a continuous outcome variable. The analysis divides data into sub-groups using binary branches, which indicates partioning of the outcome variable based predictor variables. 
  There are two benefits using the analysis. The first one is that results from the analysis are simple to understand. Another one is that we don't need to assume linearlity relationship between ouctcome and predictor variables that linear regression model has. 
  In the analysis, splits are determined by minimum residual sum of squares.That is, splits can be selected by minimization of within group sum of squares during maximization of between group sum of squares (ANOVA). This process is repeated using meaningful predictor variables that can greatly improve accuracy of the tree (least squares) until a minimum size of the end sub-groups is reached or no longer improvements in the accuracy of adding splits are showen.Thus, we need to have decision criteria for not only stopping criteria of the analysis but also having proper results without overfitting data. More details will be explained in the decision section. 

\section{Methods}

\subsection{Data filtering}

  Five categorical and six continuous variables were selected because of consistency between the other group members. The 11 variables were extracted from the original dataset and the new dataset (frm2) was used for regression tree analysis.
  
<<data (frm2)>>=
require(ggplot2)
require(RCurl)
data <- getURL("https://raw.githubusercontent.com/arvindram12/Final-Project/master/frmgham2.csv",ssl.verifypeer=0L, followlocation=1L)
writeLines(data,'framingham.csv')
frm<-read.csv('framingham.csv')
frm$RANDID<-as.factor(frm$RANDID)
frm1<-frm[which(!duplicated(frm$RANDID)),]
frm2 <- data.frame(as.factor(frm1$SEX), as.factor(frm1$DIABETES), 
              as.factor(frm1$BPMEDS),as.factor(frm1$CURSMOKE),
              as.factor(frm1$educ), frm1$AGE, frm1$TOTCHO, frm1$BMI,frm1$GLUCOSE, 
               frm1$HEARTRTE, frm1$SYSBP) 
names(frm2) <- c("SEX","DIABETES", "BPMEDS", "CURSMOKE","educ","AGE",
                    "TOTCHOL","BMI","GLUCOSE","HEARTRTE","SYSBP")
@

\subsection{Regression tree analysis}

  First, 10 variables were used to predict systolic blood pressure (SYSBP). The numbers at the terminal are mean systolic blood pressures of the number of observations falling in each terminal.To start, the initial cp value was set to 0.005, which might result in overfitting of the data. 
  There were missing values in the outcome and predictor variables. If the outcome variable has missing values, all observations from the predictors were removed. However, outcome variable values were kept when one or more predictor variables have missing values.
  
<<Regression tree>>=
require(rpart)
require(rpart.plot)

# "anova" = when outcome variable is continuous with cp=0.005
# na.action = default (explained above)
fit <- rpart(SYSBP~., data=frm2, method="anova", cp=0.005)

# Visualize tree
rpart.plot(fit,digits = 2, extra=1)

@

\subsection{Validation of the result}

  Keep adding more splits results in increase in R-squared even though the added splits could just result from adding more splits. This is what we saw in the multiple regression model. To avoid this overfitting problem, we need decision criteria on the appropriate number of splits. 
  
  The cp indicates complexity parameter that allows us to decide the number of meaningful splits (cost of adding one additional split).To be specific, higher cp values lead to smaller number of splits. we can decide the number of splits where the overall R-squared value is not increased more by increase in cp values.
  
  The minimum number of splits was defined by changes in X-val relative error as a function of cp values. After the size of tree is larger than 6, the cp values remain constant and there were no further improvements in the X-val relative error. Thus, six splits were decided here. 
  
<< Decision >>=
# Show results
printcp(fit)

# Cross-Validation results
# graphics.off()
plotcp(fit)
par(mfrow=c(1,2))
rsq.rpart(fit)

# Based on cp value, prune the tree to avoid overfitting
# from printcp(fit)
pr_fit<- prune(fit, cp=0.01) 

@

\subsection{Comparison}

  Comparison results from the original tree and the pruned tree.
<<Comparison>>=
 
par(mfrow=c(1,2), oma=c(1,1,0,0), mar=c(2,2,1,0), tcl=-0.1, mgp=c(1,0,0))
rpart.plot(fit,digits = 2, extra=1)
rpart.plot(pr_fit,digits = 2, extra=1)

# Show Summary of the pruned tree
# summary(pr_fit)
# Show residuals
summary(residuals(pr_fit))
plot(predict(pr_fit), residuals(pr_fit))

@

\subsection{Multiple linear regression}  

Comparison results from regression tree

<<Multiple regression>>=

mlr <- lm(SYSBP~AGE+BMI+BPMEDS+HEARTRTE, data=frm2)
summary(mlr)
MSE <- sum(mlr$residuals^2)/4348
MSE
@

\end{document}