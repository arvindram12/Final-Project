\documentclass{article}
\usepackage[margin=1in]{geometry}
\begin{document}
\title{Model Selection Metrics}
\author{Eric Reed}
\maketitle
\write18{wget https://github.com/arvindram12/Final-Project/blob/master/rank1.png}
\write18{wget https://github.com/arvindram12/Final-Project/blob/master/3crits.png}
<<data,  eval=FALSE,echo=FALSE>>=
require(ggplot2)
require(RCurl)
require(plyr)
data <- getURL("https://raw.githubusercontent.com/arvindram12/Final-Project/master/frmgham2.csv",ssl.verifypeer=0L, followlocation=1L)
writeLines(data,'framingham.csv')
frm<-read.csv('framingham.csv')
frm$RANDID<-as.factor(frm$RANDID)
frm$SEX<-as.factor(frm$SEX)
frm$CURSMOKE<-as.factor(frm$CURSMOKE)
frm$DIABETES<-as.factor(frm$DIABETES)
frm$BPMEDS<-as.factor(frm$BPMEDS)
frm$educ<-as.factor(frm$edu)
frm1<-frm[which(!duplicated(frm$RANDID)),]
test<-function(outcome,need,test,data){
  out1<-data[,outcome]
  need1<-as.data.frame(data[,need])
  colnames(need1)<-colnames(data[need])
  test1<-data[,test]
  x<-1:ncol(test1)
  combn1<-function(x,y){combn(y,x)}
  dee<-lapply(x, combn1, y=x)
  elem<-function(x,y,z){
    test2<-as.data.frame(x=z[,x])
    colnames(test2)<-colnames(z[x])
      mat<-as.data.frame(cbind(test2,need1))
      lm(y~.,data=mat)}
  what<-function(c){
    apply(dee[[c]],2,elem,y=out1,z=test1)}
    model<-as.list(lapply(x,what))
  model1<-unlist(model, recursive=FALSE)
  call<-function(x){
    sum<-summary(x)
    sum$adj.r.squared}
call1<-function(x){
  names(x$coef)}
  coef<-lapply(model1, call1)
coef1<-lapply(coef,rbind)

 coef2<-rbind.fill.matrix(coef1)
  adjr<-lapply(model1,call)
adjr1<-do.call(rbind,adjr)
  aics<-lapply(model1, AIC)
aics1<-do.call(rbind,aics)
  bics<-lapply(model1, BIC)
bics1<-do.call(rbind,bics)
frame<-as.data.frame<-cbind(adjr1,aics1,bics1)
colnames(frame)<-c("Adj R^2", "AIC", "BIC")
list(coef2,frame,model1)
}
yes<-test(outcome="SYSBP", need=c("AGE", "SEX"), test=c("TOTCHOL", "BMI", "GLUCOSE", "CURSMOKE", "DIABETES", "BPMEDS", "HEARTRTE", "educ"), data=frm1)  
yes1<-yes[[1]]
yes2<-yes[[2]]
rank<-cbind(nrow(yes2)+1-rank(yes2[,1]),rank(yes2[,2]), rank(yes2[,3]))
rank<-as.data.frame(rank)
yes2<-as.data.frame(yes2)
yes2[,4]<-c(rep(1,choose(8,1)),rep(2,choose(8,2)), rep(3,choose(8,3)), rep(4,choose(8,4)), rep(5,choose(8,5)), rep(6,choose(8,6)), rep(7,choose(8,7)), rep(8,choose(8,8)))
yes2[,4]<-as.factor(yes2[,4])

adjranks<-c(which(rank[,1]==min(rank[1:8,1])),
which(rank[,1]==min(rank[9:36,1])),
which(rank[,1]==min(rank[37:92,1])),
which(rank[,1]==min(rank[93:162,1])),
which(rank[,1]==min(rank[163:218,1])),
which(rank[,1]==min(rank[219:246,1])),
which(rank[,1]==min(rank[247:254,1])),
which(rank[,1]==min(rank[255:255,1])))

aicranks<-c(which(rank[,2]==min(rank[1:8,2])),
which(rank[,2]==min(rank[9:36,2])),
which(rank[,2]==min(rank[37:92,2])),
which(rank[,2]==min(rank[93:162,2])),
which(rank[,2]==min(rank[163:218,2])),
which(rank[,2]==min(rank[219:246,2])),
which(rank[,2]==min(rank[247:254,2])),
which(rank[,2]==min(rank[255:255,2])))

bicranks<-c(which(rank[,3]==min(rank[1:8,3])),
which(rank[,3]==min(rank[9:36,3])),
which(rank[,3]==min(rank[37:92,3])),
which(rank[,3]==min(rank[93:162,3])),
which(rank[,3]==min(rank[163:218,3])),
which(rank[,3]==min(rank[219:246,3])),
which(rank[,3]==min(rank[247:254,3])),
which(rank[,3]==min(rank[255:255,3])))

adjrvalues<-yes2[adjranks,]
aicbicvalues<-yes2[aicranks,]

adjrmodel<-yes1[adjranks,]
aicbicmodel<-yes1[bicranks,]

multiplot <- function(..., plotlist=NULL, file, cols=1, layout=NULL) {
  require(grid)

  # Make a list from the ... arguments and plotlist
  plots <- c(list(...), plotlist)

  numPlots = length(plots)

  # If layout is NULL, then use 'cols' to determine layout
  if (is.null(layout)) {
    # Make the panel
    # ncol: Number of columns of plots
    # nrow: Number of rows needed, calculated from # of cols
    layout <- matrix(seq(1, cols * ceiling(numPlots/cols)),
                    ncol = cols, nrow = ceiling(numPlots/cols))
  }

 if (numPlots==1) {
    print(plots[[1]])

  } else {
    # Set up the page
    grid.newpage()
    pushViewport(viewport(layout = grid.layout(nrow(layout), ncol(layout))))

    # Make each plot, in the correct location
    for (i in 1:numPlots) {
      # Get the i,j matrix positions of the regions that contain this subplot
      matchidx <- as.data.frame(which(layout == i, arr.ind = TRUE))

      print(plots[[i]], vp = viewport(layout.pos.row = matchidx$row,
                                      layout.pos.col = matchidx$col))
    }
  }
}
pl1<-ggplot(yes2, aes(x=1:255, y=yes2[,1], color=yes2[,4])) + geom_point(aes(colour=yes2[,4]),size=2)+ ggtitle("Adjusted R^2 Values of \n Multiple Linear Regression Models")+ylab("Adjusted R^2 Value")+xlab("Model Number")+ labs(colour = "No. Added \n Variables")+ theme(legend.position="none")

pl2<-ggplot(yes2, aes(x=1:255, y=yes2[,2], color=yes2[,4])) + geom_point(aes(colour=yes2[,4]),size=2)+ ggtitle("Akaike Information Criterion Values of \n Multiple Linear Regression Models")+ylab("AIC Value")+xlab("Model Number")+ labs(colour = "No. Added \n Variables")+ theme(legend.position="none")

pl3<-ggplot(yes2, aes(x=1:255, y=yes2[,3], color=yes2[,4])) + geom_point(aes(colour=yes2[,4]),size=2)+ ggtitle("Bayesian Information Criterion Values of \n Multiple Linear Regression Models")+ylab("BIC Value")+xlab("Model Number")+ labs(colour = "No. Added \n Variables")
#multiplot(pl1,pl2,pl3,cols=3)

#p <- ggplot() + 
 # geom_point(data = rank, aes(x = 1:255, y = rank[,1],shape="Adj. R^2", colour=yes2[,4])) +
  #geom_point(data = rank, aes(x = 1:255, y = rank[,2],shape="AIC", colour=yes2[,4])) +
  #geom_point(data = rank, aes(x = 1:255, y = rank[,3],shape="BIC", colour=yes2[,4])) +
  #scale_shape_manual("Selection Metric",
   #                   breaks = c("Adj. R^2", "AIC", "BIC"),
    #                  values = c(16, 0, 4)) +
#  labs(colour = "No. Added \n Variables")+
#  xlab('Model Number') +
 # ylab('Model Rank')+ ggtitle("Ranks  of Models for 3 Model Selection Metrics")
@

\section{Introduction}

  The purpose of this analysis is to test three model selection metrics, Adjusted $R^2$, Aikake Information Criterion(AIC), and Bayesion information criterion (BIC), to explore how choice of these criterion can impact model selection. We will due this by building different Multiple Linear Regression Models of different combinations of predictor variables on the outcome variable, assuming linear relationships of each variable and outcome variable.  The outcome variable tesed for was systolic blood pressure.  Every model included age and sex of the study participants. The continuous variables being tested were, total cholesterol, body mass index, blood pressure, and glucose level.  The categorical variable being tested for was current smoking status, diabetes status, whether or not the participants takes blood pressure medication, and education level.

\subsection{Adjusted $R^2$}
  Adjusted $R^2$ is a selection metric that using the equation: $$R^2_a = 1-\frac{RSS/(n-p-1)}{TSS/(n-1)}$$.  Where $RSS$ is the residual sum of squares of the predictor variables, $TSS$ in the total sum of squares of the  outcome variable, $p$ is the number of predictor variables and $n$ is the number of observations.  This metic is comprable to a typical $R^2$ calculation which is simply $$R^2=1-\frac{RSS}{TSS}$$.  What is useful abou the adjusted $R^2$ however is that it may penalize added variables that don't contribute predictive value to the model.  Predictive value of a model is assessed via the Adjusted $R^2$ value as its closeness to 1.
\subsection{Aikake Information Criterion}
 The equation for Aikake Informatino Criteria is $$ AIC=nlog(RSS/n)+2(p+1)$$.  As with adjusted $R^2$, it includes the variable for residual sum of squares, numer of variables and numberof obsevations.  As is evident by apparent differences between the two equations, the AIC measures predictive value differently.  However it functions similarly that it penalizes higher values for residual sum of squares of the predictor variables and for added variable.  Also, counter to adjusted $R^2$, lower values of AIC are indicative to increased model predictive value.
 \subsection{Bayesion Information Criterion}
 The equation for Bayesian information criterion is very similar to AIC. It is $$ BIC=nlog(RSS/n)+(p+1)log(n) $$.  Here the equation are idenitical excepte for $2(p+1)$ in AIC is replaced with $(p+1)log(n)$.  Here we can see that the two metrics handle the number of observations differently.  They will therefore yeild similar but not identical values.
 
\section{Methods}
In this analysis there were 8 variables tested along with sex and age for predictive value of systolic blood pressure.  This results in 255($2^8-1$) possible combinations of variables, and therefore 255 possible multiple linear regression models, assuming linearity of every predictor variable.  These models were generated in R using the lm() function.  Lists of variables and adjusted $R^2$ were extracted directly from ``lm()" object and AIC and BIC values were calculated for each model using the ``aic()" and ``bic()" functions. respectively. As mentioned before, higher adjusted $R^2$ values and lower AIC and BIC values are indicative to model proedictive value.  The number of variables added to the model are color coded between 1 and 8.  Here, we can notice a general trend for model predictive value to increase accross all three values. However there the distribution between the AIC and BIC are much more closely related than that of the adjusted $R^2$.

\section{Results}

\begin{figure}[h]
\begin{center}
    \centering
    \includegraphics[width=1\textwidth]{3crits.png}
    \caption{Metric Values of MLR Models, Using Three Different  Model Selection Metrics}
    \label{fig:awesome_image}
\end{center}
\end{figure}
  
  Figure 1 depicts the output of each selection metric.  The $x-axis$ represents each of the 255 models that were gerated, and the $y-axis$ represents the respective metric value of each model.  Here we can tell that there is a broad trend for predictive value to increase as the number of added variables to increase.  However, we can see that the distribution of the adjusted $R^2$ is very different when compared to that of AIC, BIC. Since each metric will output a different value accross three of the same models it is difficult to inperpret, how the different selection metrics have ranked the models by there values alone. 
  
\begin{figure}[h]
\begin{center}
    \centering
    \includegraphics[width=0.8\textwidth]{rank1.png}
    \caption{Ranks of MLR Models, Using Three Different  Model Selection Metrics}
    \label{fig:awesome_image}
\end{center}
\end{figure}  
  Figure 2 represents the ranking of each model, according to each of the three selection metrics. The values of each output are ordered from strongest predictive values, 1, to weakest predictive value, 255.  Since these ranks are on the same scale we can represent all three selection metrics are now interprettable on the same graph.  The most important point here is that each metric can assign a different rank of predictive value to the same model, though AIC and BIC will more often return the same rank for a given model.

Table 1 represents the strongest predictive value for each of the number of added variables using adjusted $R^2$. Table 2 represents the strongest predictive value for each of the number of added variables using AIC and BIC.  AIC and BIC are included in the same table as they aggreed on the highest ranked model for each case. In each table the values of the other metric is given as well. 

 



\begin{table}[ht]
\begin{tiny}
\centering
\begin{tabular}{rrllllllllrrr}
  \hline
Variables & BMI & BP Med? & Heart Rt. & Tot Chol. & Glucose & Educ. & Smoke? & Diabetes & Adj $R^2$ & AIC & BIC \\ 
  \hline
        1 & YES & NO & NO & NO & NO & NO & NO & NO & 0.237966 & 38772.70 & 38804.67 \\ 
     2 & YES & YES & NO & NO & NO & NO & NO & NO & 0.275382 & 38006.02 & 38044.29 \\ 
     3 & YES & YES & YES & NO & NO & NO & NO & NO & 0.300141 & 37844.80 & 37889.45 \\ 
     4 & YES & YES & YES & YES & NO & NO & NO & NO & 0.301882 & 37398.21 & 37449.15 \\ 
     5 & YES & YES & YES & YES & YES & NO & NO & NO & 0.30543 & 34387.96 & 34444.50 \\ 
     6 & YES & YES & YES & YES & YES & YES & NO & NO & 0.306995 & 33499.77 & 33574.84 \\ 
     7 & YES & YES & YES & YES & YES & YES & YES & NO & 0.307125 & 33500.04 & 33581.37 \\ 
     8 & YES & YES & YES & YES & YES & YES & YES & YES & 0.306962 & 33501.94 & 33589.52 \\  
   \hline
   \end{tabular}
\caption{Multiple Linear Regression Models for Maximum Adjusted $R^2$}
\end{tiny}
\end{table}

   \begin{table}[ht]
   \begin{tiny}
\centering
\begin{tabular}{rrlllllllllrr}
  \hline
  Variables & Glucose & Educ. & BP Med? & BMI & Heart Rate & Tot Chol. & Smoke? & Diab. & Adj $R^2$ & AIC & BIC \\ 
  \hline
  1 & YES & NO & NO & NO & NO & NO & NO & NO & 0.237966 & 38772.70 & 38804.67 \\ 
     2 & YES & YES & NO & NO & NO & NO & NO & NO & 0.275382 & 38006.02 & 38044.29 \\ 
     3 & YES & YES & YES & NO & NO & NO & NO & NO & 0.300141 & 37844.80 & 37889.45 \\ 
     4 & YES & YES & YES & YES & NO & NO & NO & NO & 0.301882 & 37398.21 & 37449.15 \\ 
     5 & YES & YES & YES & YES & YES & NO & NO & NO & 0.30543 & 34387.96 & 34444.50 \\ 
     6 & YES & YES & YES & YES & YES & YES & NO & NO & 0.306995 & 33499.77 & 33574.84 \\ 
     7 & YES & YES & YES & YES & YES & YES & YES & NO & 0.307125 & 33500.04 & 33581.37 \\ 
     8 & YES & YES & YES & YES & YES & YES & YES & YES & 0.306962 & 33501.94 & 33589.52 \\ 
   \hline
\end{tabular}
\caption{Multiple Linear Regression Models for Minimum AIC and BIC}
\end{tiny}
\end{table}

The main trend that every metric followed was that once a variable was added to the model, it was present in each of the following models, when a new variable was added.  For example, in the case of adjusted $R^2$ BMI was demonstrated to be the best single added variable predictor. Next, blood pressure medication status was added along with BMI to be the strongest two added variable predictors.  These two were also present in the strongest 3 added variable predictors along with heart rate.  

Ultimately the strongest model as predicted by adjusted $R^2$ differed from AIC and BIC by one variable. The strongest model predicted by adjusted $R^2$, in order of added variables was: BMI, blood pressure medication, heart rate, total cholesterol, glucose, education, and smoking status, while that predicted by AIC and BIC in order of added variables was: glucose, education, blood pressure medication, BMI, heart rate, and total cholesterol. Therefore, the addition of smoking status by adjusted $R^2$ and the lack there of in AIC and BIC is the only difference between the  strongest presumed model.

What's perhaps most striking is that if we were to try and use these same variables to predict the strongest model by adding less than 6 variables, than adjusted $R^2$ and AIC, and BIC would have yielded different models entirely.  For example the strongest three added variable predictor by adjusted $R^2$ was BMI, blood pressure medication, and heart rate,  while that of AIC and BIC was glucose, education, and blood pressure medication.



\end{document}